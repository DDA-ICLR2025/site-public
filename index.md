---
layout: default
---

<!-- # Overview -->

This is the homepage for the [ICLR 2025](https://iclr.cc/Conferences/2025) workshop on 'Decentralized, Distributed, and Asynchronous Computing for Machine 
Learning'.

Neural networks are increasingly being trained and deployed in distributed computing environments. The need for distributed computation arises not only in the context of large models or datasets, but also in settings where computation is inherently not physically localized, such as in multi-agent systems, federated learning, or edge computing. In these contexts, centralized control and synchronization are major bottlenecks for computation, and in some cases, are nearly impossible to enforce. Development of methods which relax or eliminate requirements for central control and synchronization has the potential to significantly improve scalability, efficiency, and robustness of training and inference, and expands the landscape of settings in which architectures can be trained and deployed. The aim of this workshop is to encourage collaboration and  between researchers and industry practitioners working on methods and applications for distributed, decentralized, and/or asynchronous machine learning. 

Some specific questions that this workshop aims to address include, but are not limited to:
  - How can insights and results from traditional decentralized and asynchronous algorithms be applied to machine learning?
  - How can asynchronous and/or decentralized methods be used to leverage different types of computing hardware (e.g. neuromorphic hardware)?
  - What are the effects on efficiency, scalability, and task performance that result from employing asynchronous and/or decentralized methods? How can we design robust evaluation metrics and benchmarks for decentralized and asynchronous machine learning systems to assess their performance?
  - How can decentralized methods for training and inference be leveraged for applications requiring privacy preservation?
  - In multi-agent systems, how can agents effectively coordinate collective learning without a central coordinator? What are the effects of asynchronous communication and decision making in this context?
  - What are the implications of using decentralized learning in dynamic environments where the topology of the network connecting machines may change over time?

The workshop encourages submissions which consider asynchrony or decentralization in areas including, but not limited to:
  - Federated learning 
  - Multi-agent systems
  - Edge computing on resource-constrained devices
  - Brain-inspired hardware and learning algorithms
  - Optimization theory and methods
  - Large-scale distributed training and inference


<!-- # Background

Some background.  -->

<!-- 
# Call for Papers

Etc.  -->

